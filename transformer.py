# -*- coding: utf-8 -*-
"""GPTAttempt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10l3-bQO9F0PLBqZqMwFiU_V9P0MgbK4u
"""

# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

N = 65
MODELD = 128
BLK_SIZE = 64
ITERATIONS = 1000
BATCH_SIZE = 256
HEADS = 16
NBLOCKS = 6
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DEVICE

model_path = "models/checkpoint_30000.pt"
retrain = False
model = None
optim = None
global LR
LR = 0.0001


def infinite_loader(loader):
    while True:
        for batch in loader:
            yield batch


def load_checkpoint(model_path):
    state = torch.load(model_path)

    return state


def create_data():
    text = open("input.txt").read()

    vocab = sorted(list(set(text + ".")))
    global cti, itc, N
    N = len(vocab)
    itc = dict(list(enumerate(vocab)))
    cti = {v: k for k, v in itc.items()}
    text = [cti[c] for c in text]
    X = []
    Y = []

    for i in range(len(text) - BLK_SIZE):
        X.append(text[i : i + BLK_SIZE])
        Y.append(text[i + 1 : i + BLK_SIZE + 1])

    X, Y = np.array(X), np.array(Y)
    X, Y = torch.tensor(X), torch.tensor(Y)
    X[:3], Y[:3]

    Xtr, Ytr = X[: int(len(X) * 0.9)], Y[: int(len(Y) * 0.9)]
    Xval, Yval = X[int(len(X) * 0.9) :], Y[int(len(Y) * 0.9) :]

    return Xtr, Ytr, Xval, Yval


@torch.no_grad()
def eval_loss():
    model.eval()
    mean_loss = 0
    it = 100
    mean_acc = 0
    for _ in range(it):
        ix = torch.randint(0, len(Xval), (BATCH_SIZE,))
        y, loss = model(
            Xval[ix].to(DEVICE, non_blocking=True),
            Yval[ix].to(DEVICE, non_blocking=True),
        )

        pre = torch.argmax(y, dim=1).to("cpu")
        correct = pre == Yval[ix]
        mean_loss += loss
        mean_acc += correct.float().mean()
    mean_acc /= it
    print(f"{mean_acc=}")
    mean_loss /= it
    model.train()
    return mean_loss


class TextGenerator(nn.Module):
    def __init__(self):
        super().__init__()

        self.positional_embeddings = nn.Embedding(BLK_SIZE, MODELD)
        self.token_embeddings = nn.Embedding(N, MODELD)

        self.drop = nn.Dropout(0.1)
        self.trans = nn.Sequential(*[EncoderBlock() for _ in range(NBLOCKS)])
        self.last = nn.Linear(MODELD, N)

    def forward(self, x, y=None):
        h = self.token_embeddings(x)  # B, BLK, EMB
        h = h + self.positional_embeddings(
            torch.arange(BLK_SIZE).repeat(x.shape[0], 1).to(DEVICE)
        )

        h = self.drop(h)
        h = self.trans(h)
        # h = h.view(h.shape[0], -1)

        # print("prelogits", h.shape)  # B, BLK, EMBD
        logits = self.last(h)  # B, BLK, N
        # print("postlogits", logits.shape)
        # probs = F.softmax(logits, -1) # B, BLK, N

        if y is None:
            probs = F.softmax(logits, -1)
            return probs, None
        logits = logits.permute(
            0, 2, 1
        )  # To pass the (N, C, D) with (N, D) to cross_entropy

        epsilon = 0.1

        targets_one_hot = F.one_hot(y, num_classes=N).float()

        targets_smooth = targets_one_hot * (1 - epsilon) + epsilon / N
        targets_smooth = targets_smooth.permute(
            0, 2, 1
        )  # To pass the (N, C, D) with (N, D) to cross_entropy

        # print("after permute", logits.shape)
        # y = y[:, -1]
        # print("y", y.shape)  # Should be # B, BLK -> after one hot B, BLK, N
        loss = F.cross_entropy(logits, y)
        return logits, loss

    @torch.no_grad()
    def generate(self, x):
        out = ""
        self.eval()
        x = x.to(DEVICE)
        for _ in range(1000):
            y, _ = self.forward(x)
            y = y[:, -1, :]
            chars = torch.multinomial(y.to(DEVICE), 1, replacement=False).flatten()
            out += itc[int(chars[0])]
            new_x = torch.cat(
                (x[:, 1:], chars.view(-1, 1).to(DEVICE, non_blocking=True)), 1
            ).to(DEVICE)
            # print(x, "\t", new_x)
            x = new_x
        print(out)
        self.train()
        print("")


class EncoderBlock(nn.Module):
    def __init__(self):
        super().__init__()

        self.linear = nn.Linear(MODELD, MODELD)

        self.attentions = MultiHead()
        self.drop1 = nn.Dropout(0.1)
        self.norm1 = nn.BatchNorm1d(BLK_SIZE)

        self.fdfd = FeedForward()
        self.drop2 = nn.Dropout(0.1)
        self.norm2 = nn.BatchNorm1d(BLK_SIZE)

    def forward(self, x):
        h1 = self.linear(x)  # B, BLK, EMB
        h2 = self.attentions(h1)  # B, BLK, EMB
        h2 = self.drop1(h2)
        h3 = self.norm1(h2 + h1)

        h4 = self.fdfd(h3)
        h4 = self.drop2(h4)
        h5 = self.norm2(h4 + h3)
        return h5


class DotProductAttention(nn.Module):
    def __init__(self):
        super().__init__()

        self.key = nn.Linear(MODELD, MODELD // HEADS)
        self.query = nn.Linear(MODELD, MODELD // HEADS)
        self.value = nn.Linear(MODELD, MODELD // HEADS)

    def forward(self, x):
        k = self.key(x)  # 8x32
        q = self.query(x)  # 8x32
        v = self.value(x)  # 8x32
        weight = (k @ q.transpose(-2, -1)) * (MODELD // HEADS) ** -0.5

        tril = torch.tril(torch.ones(BLK_SIZE, BLK_SIZE)).to(DEVICE, non_blocking=True)

        weight.masked_fill_(tril == 0, float("-inf"))
        weight.shape

        weight = F.softmax(weight, -1)
        h = weight @ v
        return h


class MultiHead(nn.Module):
    def __init__(self):
        # BATCH, BLOCK, EMBD
        super().__init__()
        self.attentions = nn.ModuleList(
            [DotProductAttention().to(DEVICE) for head in range(HEADS)]
        )
        self.fc = nn.Linear(MODELD, MODELD)

        self.relu = nn.ReLU()

    def forward(self, x):
        h = torch.cat([attention(x) for attention in self.attentions], dim=-1)
        h = self.relu(self.fc(h))

        return h


class FeedForward(nn.Module):
    def __init__(self):
        super().__init__()
        self.seq = nn.Sequential(
            # nn.Linear(embd, embd), nn.ReLU(), nn.Linear(embd, embd)
            # nn.Conv2d(batch_size, batch_size * 4, (1, 1)),
            # nn.ReLU(),
            # nn.Conv2d(batch_size * 4, batch_size, (1, 1)),
            nn.Linear(MODELD, MODELD * 4),
            nn.ReLU(),
            nn.Linear(MODELD * 4, MODELD),
        )

    def forward(self, x):
        # x = x.unsqueeze(0)
        x = self.seq(x)
        # x = x.squeeze(0)
        return x


def train(lr):
    # optim = torch.optim.Adam(model.parameters(), lr=lr)
    model.train()
    running_loss = 0

    for i in range(1, 100000):
        # ix = torch.randint(0, len(Xtr), (batch_size,))
        # xin, yin = Xtr[ix], Ytr[ix]
        xin, yin = next(inf_loader)
        xin, yin = xin.to(DEVICE, non_blocking=True), yin.to(DEVICE, non_blocking=True)

        lr = (MODELD**-0.5) * min(i**-0.5, i * 4000**-1.5)
        for p in optim.param_groups:
            p["lr"] = lr

        _, loss = model(xin, yin)
        # if i % 10000 == 0:
        #     lr /= 10

        optim.zero_grad()
        loss.backward()
        optim.step()
        running_loss += loss.item()

        if i % 1000 == 0:
            running_loss /= 1000
            eloss = eval_loss()
            elosses.append(eloss)
            print(f"{i}: {running_loss=} \t {eloss=} \t {lr}")
            state = {"model": model.state_dict(), "optimizer": optim.state_dict()}
            torch.save(state, f"models/checkpoint_{i}.pt")

            running_loss = 0
        losses.append(loss.item())
    return losses, elosses


# model.generate(X[:1])
# x = Xval[:1].to(device)
# model.to(device)
# out = ""
# model.eval()
# for _ in range(300):
#     y, _ = model.forward(x)
#     char = torch.multinomial(y.to("cpu"), 1, replacement=False).flatten()
#     out += itc[int(char[0])]

#     new_x = torch.cat((x[:, 1:], torch.tensor([[char]]).to(device)), 1).to(device)
#     # print(x, "\t", new_x)
#     x = new_x
# print(out)
# model.train()

# print("")

"""### Log loss
* Full basic bigram with linear + emb: 2.8
* Basic 1 attention head with batchnorm: 2.1935, 2.27
* With positinal embeddings and more iterations: 2.1804
* With multi head 2.10
* With dynamic lr, 30000 iterations, and FDFD layer. 2.0427
* With residuals: 2.0294
* With Nblocks 2: 1.91
"""

# key = nn.Linear(10, 10)
# query = nn.Linear(10, 10)
# value = nn.Linear(10, 10)

# x = torch.randn((10, 10))

# k = key(x)
# q = query(x)
# print(k.shape, q.shape, q.T.shape)

# weight = k @ q.T
# weight
# tril = torch.tril(torch.ones(10, 10))

# weight.masked_fill_(tril == 0, float("-inf"))
# weight.shape

# weight = F.softmax(weight, -1)
# logits = x @ weight
# logits


if __name__ == "__main__":
    Xtr, Ytr, Xval, Yval = create_data()

    train_dataset = torch.utils.data.TensorDataset(Xtr, Ytr)
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        pin_memory=True,  # âœ… this makes .to(device) much faster
        num_workers=4,  # use multiple CPU workers
        prefetch_factor=2,
        drop_last=True,
    )

    inf_loader = infinite_loader(train_loader)

    losses = []
    elosses = []

    model = TextGenerator().to(DEVICE)

    if retrain is True:
        print(f"---- Retrain with {model_path=} ----")
        state = load_checkpoint(model_path)
        model.load_state_dict(state["model"])

    optim = torch.optim.Adam(model.parameters(), lr=LR)

    if retrain is True:
        optim.load_state_dict(state["optimizer"])

    devices = {p.device for p in model.parameters()}
    print(devices)

    losses, elosses = train(LR)

    losses_t = torch.tensor(losses)
    elosses_t = torch.tensor(elosses)

    losses_t = losses_t.view(-1, 100).mean(-1)
    print(elosses_t.shape, elosses_t.shape[0])

    print(eval_loss())
    plt.plot(losses_t)
    plt.plot(
        torch.arange(elosses_t.shape[0]) * (losses_t.shape[0] / elosses_t.shape[0]),
        elosses_t,
    )
